# Inherit configs from the default ssd300
import torchvision
from ssd.data import TDT4265Dataset
from ssd.modeling import backbones, AnchorBoxes
from ssd.modeling.focal_loss import FocalLoss
from tops.config import LazyCall as L
from ssd.data.transforms import (
    ToTensor, RandomHorizontalFlip, RandomSampleCrop, Normalize, Resize,
    GroundTruthBoxesToAnchors)
from .ssd300 import train, optimizer, schedulers, model, data_train, data_val, loss_objective
from .utils import get_dataset_dir
from ssd.modeling.retinanet import RetinaNet


##############################################################
# configuration for using SSD on the TDT4265 project dataset #
##############################################################


# Keep the model, except change the backbone and number of classes
train.imshape = (128, 1024)
train.image_channels = 3
model.num_classes = 8 + 1  # Add 1 for background class

# backbone = L(backbones.BasicModel)(
#     output_channels=[128, 256, 128, 128, 64, 64],
#     image_channels="${train.image_channels}",
#     output_feature_sizes="${anchors.feature_sizes}"
# )

backbone = L(backbones.RetinaNet)(
    output_channels=[256] * 6,
    image_channels="${train.image_channels}",
    output_feature_sizes="${anchors.feature_sizes}"
)

model = L(RetinaNet)(
     feature_extractor="${backbone}",
     anchors="${anchors}",
     loss_objective="${loss_objective}",
     num_classes=8 + 1,  # Add 1 for background
     anchor_prob_initialization=False,
     anchor_background_prob=0.99 #p in formula
)

loss_objective = L(FocalLoss)(anchors="${anchors}", alpha=[10, *[1000 for i in range(model.num_classes-1)]], gamma=2)

anchors = L(AnchorBoxes)(
    feature_sizes=[[32, 256], [16, 128], [8, 64], [4, 32], [2, 16], [1, 8]],
    # Strides is the number of pixels (in image space) between each spatial position in the feature map
    strides=[[4, 4], [8, 8], [16, 16], [32, 32], [64, 64], [128, 128]],
    min_sizes=[[16, 16], [32, 32], [48, 48], [64, 64], [86, 86], [128, 128], [128, 400]],
    # aspect ratio is defined per feature map (first index is largest feature map (38x38))
    # aspect ratio is used to define two boxes per element in the list.
    # if ratio=[2], boxes will be created with ratio 1:2 and 2:1
    # Number of boxes per location is in total 2 + 2 per aspect ratio
    aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],
    image_shape="${train.imshape}",
    scale_center_variance=0.1,
    scale_size_variance=0.2
)

train_cpu_transform = L(torchvision.transforms.Compose)(transforms=[
    L(RandomSampleCrop)(),
    L(ToTensor)(),
    L(RandomHorizontalFlip)(),
    L(Resize)(imshape="${train.imshape}"),
    L(GroundTruthBoxesToAnchors)(anchors="${anchors}", iou_threshold=0.5),
])
val_cpu_transform = L(torchvision.transforms.Compose)(transforms=[
    L(ToTensor)(),
    L(Resize)(imshape="${train.imshape}"),
])
gpu_transform = L(torchvision.transforms.Compose)(transforms=[
    L(Normalize)(mean=[0.4765, 0.4774, 0.2259], std=[0.2951, 0.2864, 0.2878])
])
data_train.dataset = L(TDT4265Dataset)(
    img_folder=get_dataset_dir("tdt4265_2022"),
    transform="${train_cpu_transform}",
    annotation_file=get_dataset_dir("tdt4265_2022/train_annotations.json"))
data_val.dataset = L(TDT4265Dataset)(
    img_folder=get_dataset_dir("tdt4265_2022"),
    transform="${val_cpu_transform}",
    annotation_file=get_dataset_dir("tdt4265_2022/val_annotations.json"))
data_val.gpu_transform = gpu_transform
data_train.gpu_transform = gpu_transform

label_map = {idx: cls_name for idx,
             cls_name in enumerate(TDT4265Dataset.class_names)}
